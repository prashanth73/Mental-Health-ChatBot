{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD736VO1W5Fs"
      },
      "outputs": [],
      "source": [
        "#Sever Link for the website\n",
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDyKwpTKJobN"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, render_template, request, jsonify\n",
        "import re\n",
        "import torch\n",
        "import pickle\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline\n",
        "from peft import PeftModel, PeftConfig\n",
        "from huggingface_hub import login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Place your Hugging Face Access Token Here\n",
        "login(token = \"HF_TOKEN\")"
      ],
      "metadata": {
        "id": "ZnOxoglp_wAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Index.html file path.\n",
        "app = Flask(__name__, template_folder=\"/content/templates\")"
      ],
      "metadata": {
        "id": "tVjT6fedtpV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUtXE3G1JoZB"
      },
      "outputs": [],
      "source": [
        "with app.app_context():\n",
        "\n",
        "    global response_model, response_tokenizer, crisis_model, crisis_tokenizer, emotion_classifier, emotion_classifier_tokenizer, emotion_classifier_model\n",
        "\n",
        "\n",
        "    # Load response model and tokenizer\n",
        "    peft_model_id = \"Prashanth74/Gemma_2b_Response_Generation_Model_MentalHealth_Chatbot\"\n",
        "    peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
        "    response_tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
        "\n",
        "    new_tokens = [\"<extra_token_1>\", \"<extra_token_2>\"]\n",
        "    response_tokenizer.add_tokens(new_tokens)\n",
        "    #print(\"Tokenizer size after adding tokens:\", len(response_tokenizer))  # Should now be 256002\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        peft_config.base_model_name_or_path,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    base_model.resize_token_embeddings(len(response_tokenizer))\n",
        "    response_model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
        "    print(\"Response Model and Tokenizer Loaded Sucessfully....\")\n",
        "    #response_model.eval()\n",
        "\n",
        "    # Load crisis detection model\n",
        "    crisis_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    crisis_model = AutoModelForSequenceClassification.from_pretrained(\"Prashanth74/BERT_Crisis_Detection_Model\")\n",
        "    print(\"Crisis Model and Tokenizer Loaded Sucessfully....\")\n",
        "    #crisis_model.eval()\n",
        "\n",
        "    #Load emotion classifier model\n",
        "    emotion_classifier_tokenizer = AutoTokenizer.from_pretrained(\"nateraw/bert-base-uncased-emotion\")\n",
        "    emotion_classifier_model = AutoModelForSequenceClassification.from_pretrained(\"Prashanth74/BERT_Emotion_Classification_Model\")\n",
        "\n",
        "    emotion_classifier = pipeline(\"text-classification\", model=emotion_classifier_model, tokenizer=emotion_classifier_tokenizer)\n",
        "    print(\"Emotion Classifier Model and Tokenizer Loaded Sucessfully....\")\n",
        "    #emotion_classifier.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XJMr4G7Kkcq"
      },
      "outputs": [],
      "source": [
        "emotion_labels = ['example_very_unclear',\n",
        " 'admiration',\n",
        " 'amusement',\n",
        " 'anger',\n",
        " 'annoyance',\n",
        " 'approval',\n",
        " 'caring',\n",
        " 'confusion',\n",
        " 'curiosity',\n",
        " 'desire',\n",
        " 'disappointment',\n",
        " 'disapproval',\n",
        " 'disgust',\n",
        " 'embarrassment',\n",
        " 'excitement',\n",
        " 'fear',\n",
        " 'gratitude',\n",
        " 'grief',\n",
        " 'joy',\n",
        " 'love',\n",
        " 'nervousness',\n",
        " 'optimism',\n",
        " 'pride',\n",
        " 'realization',\n",
        " 'relief',\n",
        " 'remorse',\n",
        " 'sadness',\n",
        " 'surprise',\n",
        " 'neutral']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []"
      ],
      "metadata": {
        "id": "VD5oZm-_r1un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaZXFDIDJoWS"
      },
      "outputs": [],
      "source": [
        "#Removing incomplete sentences from model response\n",
        "def clean_response(text):\n",
        "    sentences = re.findall(r'[^.]*\\.', text)\n",
        "    return ''.join(sentences).strip()\n",
        "\n",
        "#Mapping the user_input to corresponding emotion label\n",
        "def convert_label_to_emotion(predicted_emotion):\n",
        "\n",
        "    label_to_emotion = {f\"LABEL_{i}\": emotion for i, emotion in enumerate(emotion_labels)}\n",
        "\n",
        "    emotion_scores = {}\n",
        "    for item in predicted_emotion:\n",
        "        label = item['label']\n",
        "        score = item['score']\n",
        "\n",
        "        if label in label_to_emotion:\n",
        "            emotion = label_to_emotion[label]\n",
        "            emotion_scores[emotion] = score\n",
        "        else:\n",
        "            print(f\"Warning: Label '{label}' not found in the label mapping.\")\n",
        "\n",
        "    return emotion_scores\n",
        "\n",
        "\n",
        "#Classify the predictied emotion label to positive, negative or mixed class\n",
        "def classify_emotion(emotion_scores):\n",
        "\n",
        "    positive_emotions = [\"admiration\", \"amusement\", \"approval\", \"caring\", \"curiosity\", \"desire\", \"excitement\", \"gratitude\", \"joy\", \"love\", \"optimism\", \"pride\", \"relief\"]\n",
        "    negative_emotions = [\"anger\", \"annoyance\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\", \"fear\", \"grief\", \"nervousness\", \"remorse\", \"sadness\"]\n",
        "    mixed_emotions = [\"example_very_unclear\", \"confusion\", \"neutral\", \"surprise\", \"realization\"]\n",
        "\n",
        "    positive_score = sum(emotion_scores.get(emotion, 0) for emotion in positive_emotions)\n",
        "    negative_score = sum(emotion_scores.get(emotion, 0) for emotion in negative_emotions)\n",
        "    neutral_score = sum(emotion_scores.get(emotion, 0) for emotion in mixed_emotions)\n",
        "\n",
        "    if positive_score > negative_score:\n",
        "        return \"positive\"\n",
        "    elif negative_score > positive_score:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"mixed\"\n",
        "\n",
        "# Crisis detection\n",
        "def is_crisis(text):\n",
        "    inputs = crisis_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = crisis_model(**inputs)\n",
        "        prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
        "    return prediction == 1\n",
        "\n",
        "# Generate chatbot reply\n",
        "def generate_response(user_input):\n",
        "\n",
        "    global chat_history\n",
        "\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    prompt = response_tokenizer.apply_chat_template(chat_history, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = response_tokenizer(prompt,\n",
        "                                return_tensors='pt',\n",
        "                                padding=True,\n",
        "                                truncation=True,\n",
        "                                max_length=1024).to(response_model.device)\n",
        "\n",
        "    outputs = response_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=50,\n",
        "        do_sample=True,\n",
        "        top_k=30,\n",
        "        top_p=0.85,\n",
        "        temperature=0.7,\n",
        "        #top_k=50,\n",
        "        #top_p=0.9,\n",
        "        #repetition_penalty=1.1,\n",
        "        #eos_token_id=response_tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    decoded = response_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    if \"model\" in decoded:\n",
        "        reply = decoded.split(\"model\")[-1].strip()\n",
        "    else:\n",
        "        reply = decoded\n",
        "\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": reply})\n",
        "\n",
        "    return clean_response(reply)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fju9gdXJoU_"
      },
      "outputs": [],
      "source": [
        "@app.route(\"/\", methods=[\"GET\"])\n",
        "def index():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route(\"/chat\", methods=[\"POST\"])\n",
        "def chat():\n",
        "\n",
        "    global chat_history\n",
        "\n",
        "    user_input = request.json[\"message\"]\n",
        "\n",
        "    if user_input.lower() == \"exit\":\n",
        "\n",
        "        chat_history = []\n",
        "\n",
        "        return jsonify({\"response\": \"Goodbye! Take care. ðŸ’™\",\n",
        "                        \"emotion\":\"neutral\",\n",
        "                        \"end\":True})\n",
        "\n",
        "    if is_crisis(user_input):\n",
        "        return jsonify({\"response\": \"\\ud83d\\udcac It seems like you're going through a tough time. You're not alone. Please reach out to a crisis helpline: 988 (US National Crisis Line) or talk to someone you trust. \\ud83d\\udc99\",\n",
        "                        \"emotion\": \"negative\",\n",
        "                        \"end\": False})\n",
        "\n",
        "    raw_emotion = emotion_classifier(user_input)\n",
        "    emotion_scores = convert_label_to_emotion(raw_emotion)\n",
        "    emotion_class = classify_emotion(emotion_scores)\n",
        "\n",
        "    response = generate_response(user_input)\n",
        "    return jsonify({\"response\": response,\n",
        "                    \"emotion\": emotion_class,\n",
        "                    \"end\": False})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l8J118mormJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Crkp_N30rmHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4ku45JHrmFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ZiaZ4lBrmDJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}